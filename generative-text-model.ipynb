{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3521,"sourceType":"datasetVersion","datasetId":2075}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk, os\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nsave_dir =  \"/kaggle/input/gutenberg/gutenberg\"\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Function to preprocess text data\ndef preprocess_text(text):\n    # Tokenize text\n    tokens = word_tokenize(text.lower())\n\n    # Remove stopwords from text data\n    stop_words = set(stopwords.words('english'))\n    filtered_tokens = [word for word in tokens if word not in stop_words]\n\n    return ' '.join(filtered_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T05:36:44.062415Z","iopub.execute_input":"2024-03-24T05:36:44.063307Z","iopub.status.idle":"2024-03-24T05:36:45.959320Z","shell.execute_reply.started":"2024-03-24T05:36:44.063267Z","shell.execute_reply":"2024-03-24T05:36:45.958244Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"corpus = ''\n\n# Files with issues\nnot_to_read  = [\"shakespeare-caesar.txt\", \"README\",  \"chesterton-ball.txt\"]\n\n# Specific file(s) to read\nto_read  = [\"austen-emma.txt\"]\n\n# Preprocessing book(s) and combining them into a single corpus\nfor file in os.listdir(save_dir):\n#     if file not in not_to_read:\n    if file in to_read:\n        print(file)\n        with open(os.path.join(save_dir, file), 'r') as f:\n            text = f.read()\n            preprocessed_text = preprocess_text(text)\n            corpus += preprocessed_text + ' '","metadata":{"execution":{"iopub.status.busy":"2024-03-24T05:36:45.961250Z","iopub.execute_input":"2024-03-24T05:36:45.961699Z","iopub.status.idle":"2024-03-24T05:36:48.185880Z","shell.execute_reply.started":"2024-03-24T05:36:45.961670Z","shell.execute_reply":"2024-03-24T05:36:48.184813Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"austen-emma.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"# Saving the preprocessed corpus in txt file named \"preprocessed_corpus\"\nwith open('preprocessed_corpus.txt', 'w') as f:\n    f.write(corpus)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T05:36:48.187269Z","iopub.execute_input":"2024-03-24T05:36:48.187988Z","iopub.status.idle":"2024-03-24T05:36:48.194083Z","shell.execute_reply.started":"2024-03-24T05:36:48.187949Z","shell.execute_reply":"2024-03-24T05:36:48.192511Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Tokenizing the corpus using Tokenizer from keras\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([corpus])\ntotal_words = len(tokenizer.word_index) + 1\n\n# Creating input sequences using a sliding window approach\ninput_sequences = []\nfor line in corpus.split('\\n'):\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T05:36:48.196393Z","iopub.execute_input":"2024-03-24T05:36:48.196771Z","iopub.status.idle":"2024-03-24T05:37:45.375889Z","shell.execute_reply.started":"2024-03-24T05:36:48.196744Z","shell.execute_reply":"2024-03-24T05:37:45.374996Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-03-24 05:36:50.009996: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-24 05:36:50.010128: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-24 05:36:50.139817: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Padding sequences to have uniform length for  all data\nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n\n# Creating predictors and labels\npredictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n\n# Converting labels to one-hot encoding\nlabel = tf.keras.utils.to_categorical(label, num_classes=total_words)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T05:37:45.377131Z","iopub.execute_input":"2024-03-24T05:37:45.377858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LSTM RNN model with final dense layer\nmodel = tf.keras.Sequential([\n    Embedding(total_words, 100, input_length=max_sequence_len-1),\n    LSTM(150),\n    Embedding(total_words, 100, input_length=max_sequence_len-1),\n    LSTM(150),\n    Dense(total_words, activation='softmax')\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compiling the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model\nmodel.fit(predictors, label, epochs=100, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}